{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from keybert import KeyBERT\n",
    "keywords_model = KeyBERT(model=nlp)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('abstract.txt', 'r') as f:\n",
    "    abstract = f.read()\n",
    "with open('introduction.txt','r') as f:\n",
    "    introduction = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of docs\n",
    "### In this notebook I will try to compute similarity for two docs:\n",
    "\n",
    "- \"Attention is all you need\" paper from https://arxiv.org/pdf/1706.03762.pdf (Abstract)\n",
    "- https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09 (Introduction)\n",
    "\n",
    " According to our intuition, we can assume that both docs should be quite similar, as describe the same topic, but<br> let's see what will be the result of our similarity measure due to other styles of writing, different length of docs, etc.\n",
    " <br>\n",
    " <br>\n",
    " As first step I will do simple normalization of docs using spacy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    doc = nlp(text)\n",
    "    norm_text = []\n",
    "    for token in doc:\n",
    "        #deleting punctuations, stopwords, spaces\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "            norm_text.append(token.lemma_.lower())\n",
    "    return ' '.join(norm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_abstract = normalize(abstract)\n",
    "normalized_introduction = normalize(introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method - computing cosine simmilarity with tf-idf vectors\n",
    "#### Pros:\n",
    "- quite easy to understand\n",
    "- fast\n",
    "- statistical method - no need to see bigger corpus of docs to compare\n",
    "- when working on small datasets vectors are quite short, so fast to compute\n",
    "\n",
    "#### Cons:\n",
    "- word similarities are overlooked \n",
    "- much longer vectors when dataset is bigger and bigger, may much slow down computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_similarity(*args: str) -> None:\n",
    "    \"\"\"Compute cosine similarity between each text\"\"\"\n",
    "    # Create tf-idf matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=nlp.Defaults.stop_words)\n",
    "    tfidf_texts = tfidf_vectorizer.fit_transform(args)\n",
    "    # Compute cosine similarity for each pair of texts\n",
    "    for pair in list(itertools.combinations(range(len(args)), 2)):\n",
    "        print(f'text {pair[0]+1} and text {pair[1]+1} has cosine similarity {np.round(cosine_similarity(tfidf_texts[pair[0]], tfidf_texts[pair[1]])[0][0],2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1 and text 2 has cosine similarity 0.11\n"
     ]
    }
   ],
   "source": [
    "tfidf_cosine_similarity(normalized_abstract, normalized_introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method - computing cosine simmilarity with spacy embeddings vectors\n",
    "#### Pros:\n",
    "- embeddings can maintain interrelationships of words\n",
    "- we can compare short docs with quite good output\n",
    "\n",
    "#### Cons:\n",
    "- rather hard to understand - vectors are created by neural network usually with Transformer architecture (BERT, roBERTa, etc)\n",
    "- output is as good as data used to train embeddings - when working on very specific data we may need to train our own transformer to create embeddings - this is really hard to do on personal computers, nearly impossible\n",
    "\n",
    "Although we can find a lot OpenSource Transformers on HuggingFace, GitHub, etc.\n",
    "Here I will use \"big english spacy\", but there is always space to improve method by testing other embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_cosine_similarity(*args: str) -> None:\n",
    "    \"\"\"Compute cosine similarity between each text\"\"\"\n",
    "    # Create spacy vectors\n",
    "    spacy_texts = [nlp(text) for text in args]\n",
    "    # Compute cosine similarity for each pair of docs\n",
    "    for pair in list(itertools.combinations(range(len(args)), 2)):\n",
    "        print(f'text {pair[0]+1} and text {pair[1]+1} has cosine similarity {np.round(spacy_texts[pair[0]].similarity(spacy_texts[pair[1]]),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1 and text 2 has cosine similarity 0.91\n"
     ]
    }
   ],
   "source": [
    "spacy_cosine_similarity(normalized_abstract, normalized_introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third method - comparing keywords\n",
    "#### Pros:\n",
    "- easy (especialy when using vectorizers like tf-idf)\n",
    "\n",
    "#### Cons:\n",
    "- can work only as faciliation for further human validation\n",
    "- if we want to use tf-idf there has to be good corpus to create good keywords for document\n",
    "\n",
    "Due to above cons, I use here KeyBert method to create keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keywords(doc: str) -> list:\n",
    "    \"\"\"Create sorted list of 10 keywords\"\"\"\n",
    "    keywords = keywords_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=nlp.Defaults.stop_words, top_n=20)\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "    \n",
    "def compare_keywords(*args: str) -> None:\n",
    "    # Create keywords\n",
    "    keywords = [create_keywords(text) for text in args]\n",
    "    # Compute cosine similarity for each pair of docs\n",
    "    for pair in list(itertools.combinations(range(len(args)), 2)):\n",
    "        print(f'text {pair[0]+1} and text {pair[1]+1} has common keywords: {set(keywords[pair[0]]).intersection(set(keywords[pair[1]]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1 and text 2 has common keywords: {'base', 'architecture', 'state'}\n"
     ]
    }
   ],
   "source": [
    "compare_keywords(normalized_abstract, normalized_introduction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    " <br><br>\n",
    "I wanted to show how big improvement are transformers for whole NLP - comparing results of cosine similarity with other vectors (tf-idf vs embeddings) is quite good for that: \n",
    "<br><br>\n",
    "In simple statistical method we get 0.11, so the documents are not similar - this may be cause by lenght of text and other styles of writing of papers and articles. \n",
    "<br>\n",
    "When comparing it to results of cosine similarity computed with embeddings vector we can see completely different results - 0.91 score. The reason of that is that making embeddings vectors by transformers maintains dependence in distance. For example words like \"queen\" and \"king\" will be closer than words like \"bike\" and \"grenade\". \n",
    "<br><br><br>\n",
    "This notebook is rather simple presentation of mehtods we can use for NLP, there is plenty space to improve all above methods mainly by parameters tuning. One quite interesting way is using ngrams (phrases) rather than only words.<br><br>\n",
    "Although in my opinion this is the biggest challenge for unsupervised approaches to NLP - finding good validation method and tuning our models/scripts to this. There is never one the best way to compare above methods. This should rather comes from creating application requirements and way it will be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff1915054b027d51168994c598795f952fcc2755ace881bae9dbebd3616c1f59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
